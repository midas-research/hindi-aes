{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzLgZebfCRd_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "from sklearn import datasets, linear_model, metrics\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqEAaaUY9xAT"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def LR2():\n",
        "  # a=x\n",
        "  # try:\n",
        "  name=\"/content/organic\"\n",
        "  name=name+'.csv'\n",
        "  df = pd.read_csv(name)\n",
        "  # df.overlap_score*=1000\n",
        "  df.overlap_score= (df.overlap_score-df.overlap_score.min())/(df.overlap_score.max()-df.overlap_score.min())\n",
        "  df.unique_words_count= (df.unique_words_count-df.unique_words_count.min())/(df.unique_words_count.max()-df.unique_words_count.min())\n",
        "  df.essay_length= (df.essay_length-df.essay_length.min())/(df.essay_length.max()-df.essay_length.min())\n",
        "  df.average_sentence_length= (df.average_sentence_length-df.average_sentence_length.min())/(df.average_sentence_length.max()-df.average_sentence_length.min())\n",
        "  df.average_word_length= (df.average_word_length-df.average_word_length.min())/(df.average_word_length.max()-df.average_word_length.min())\n",
        "  df.readability= (df.readability-df.readability.min())/(df.readability.max()-df.readability.min())\n",
        "  x=df[['unique_words_count','overlap_score','essay_length','average_sentence_length','average_word_length','readability']]\n",
        "  y=df['score']\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4, random_state=42)\n",
        "\n",
        "  x_test, x_valid, y_test, y_valid = train_test_split(x_test, y_test, test_size = 0.5, random_state=42)\n",
        "\n",
        "  # clf = () \n",
        "  # # clf.fit(x_train, y_train)\n",
        "  # # y_pred = clf.predict(x_test)\n",
        "  # # grid_rf = GridSearchCV(estimator = clf, param_grid = random_grid, cv = 3, verbose=2, n_jobs = -1)\n",
        "  # # Fit the random search model\n",
        "  # grid_rf.fit(x_train, y_train)\n",
        "  # print(grid_rf.best_params_)\n",
        "\n",
        "  # clf = grid_rf.best_estimator_\n",
        "  lr = LinearRegression()\n",
        "  lr.fit(x_train, y_train)\n",
        "  y_pred = lr.predict(x_test)\n",
        "  # print(y_pred)\n",
        "  # reg = linear_model.LinearRegression()\n",
        "  # reg.fit(x_train, y_train)\n",
        "  # y_pred = reg.predict(x_test)\n",
        "  y_pred=np.around(y_pred,0)\n",
        "  # # print(y_pred)\n",
        "\n",
        "  qwk=metrics.cohen_kappa_score(y_test, y_pred, labels=None, weights='quadratic', sample_weight=None)\n",
        "  print(\"Quadratic Weighted Kappa of prompt organic\"+\" \"+str(qwk))\n",
        "  # except:\n",
        "  #   print(\"Error in prompt \"+str(a))"
      ],
      "metadata": {
        "id": "HQUT-oN7UoGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR2()"
      ],
      "metadata": {
        "id": "6w3_HUMiUppn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}