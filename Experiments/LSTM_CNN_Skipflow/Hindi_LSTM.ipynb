{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git"
      ],
      "metadata": {
        "id": "qVUgY6PGdan6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git"
      ],
      "metadata": {
        "id": "chrafvoKeQL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES=r\"/content/indic_nlp_resources\""
      ],
      "metadata": {
        "id": "3InPluFrehRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "id": "q3Urllx_fAr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget  https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/embedding-v2/indicnlp.ft.hi.300.bin"
      ],
      "metadata": {
        "id": "GFtPI7USfCSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.hi.zip"
      ],
      "metadata": {
        "id": "L-Zc0tJYhG7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/wiki.hi.zip'"
      ],
      "metadata": {
        "id": "UuZFpwDchkIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "embedding_model = fasttext.load_model('/content/wiki.hi.bin')"
      ],
      "metadata": {
        "id": "wG4ZeGqmfMlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/calling-out-bluff/Model3(SkipFlow)/"
      ],
      "metadata": {
        "id": "oPXW7q5ofQX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import  keras.layers  as  klayers \n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, LSTM, Input, Embedding, GlobalAveragePooling1D, Concatenate, Activation, Lambda, BatchNormalization, Convolution1D, Dropout, Bidirectional\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import nltk\n",
        "from quadratic_weighted_kappa import QWK\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer, InputSpec\n",
        "from keras.optimizers import adam_v2\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import regularizers\n",
        "from keras import initializers\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "ya1xx647iG7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM=300\n",
        "MAX_NB_WORDS=4000\n",
        "\n",
        "MAX_SEQUENCE_LENGTH=500\n",
        "VALIDATION_SPLIT=0.20\n",
        "DELTA=20\n",
        "\n",
        "texts=[]\n",
        "\n",
        "labels=[]\n",
        "\n",
        "\n",
        "originals = []"
      ],
      "metadata": {
        "id": "uwnlC8tRiIoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/Untitled spreadsheet - Sheet1 (2).csv')"
      ],
      "metadata": {
        "id": "ImBWPP_XTGk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "qzF7PONQTm3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('Unnamed: 18',axis=1)\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "5E8fV41PUHXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df)):\n",
        "  originals.append(df['score'].iloc[i])"
      ],
      "metadata": {
        "id": "CpLEAPkKUNFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "essay_type = '1'\n",
        "\n",
        "fp=open(\"/content/drive/MyDrive/promt_1_1783 - Sheet1.tsv\",'r', encoding=\"utf-8\", errors=\"ignore\")\n",
        "fp.readline()\n",
        "originals = []"
      ],
      "metadata": {
        "id": "VS0QSW8miX73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in fp:\n",
        "    temp=line.split(\"\\t\")\n",
        "    if(temp[2]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
        "        originals.append(float(temp[4]))\n",
        "# print(originals)\n",
        "fp.close()\n",
        "# print(originals)\n",
        "print(\"range min - \", min(originals) , \" ; range max - \", max(originals))\n"
      ],
      "metadata": {
        "id": "fVdfdEdwjjZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"range min - \", min(originals) , \" ; range max - \", max(originals))"
      ],
      "metadata": {
        "id": "Ju40oUqNVCfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))"
      ],
      "metadata": {
        "id": "N7IrM3Yfj3JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.tokenize import indic_tokenize  \n",
        "\n",
        "range_min = min(originals)\n",
        "range_max = max(originals)\n",
        "sentences=[]\n",
        "for i in range(len(df)):\n",
        "  texts.append(df['essay_hindi'].iloc[i])\n",
        "  labels.append(((df['score'].iloc[i]-range_min)/(range_max-range_min)))\n",
        "  line = df['essay_hindi'].iloc[i].strip()\n",
        "  sentences.append(indic_tokenize.trivial_tokenize(line))"
      ],
      "metadata": {
        "id": "BXLMpN9oVPEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.tokenize import indic_tokenize  \n",
        "\n",
        "range_min = min(originals)\n",
        "range_max = max(originals)\n",
        "\n",
        "fp=open(\"/content/drive/MyDrive/promt_1_1783 - Sheet1.tsv\",'r', encoding=\"utf-8\", errors=\"ignore\")\n",
        "fp.readline()\n",
        "sentences=[]\n",
        "for line in fp:\n",
        "    temp=line.split(\"\\t\")\n",
        "    if(temp[2]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
        "        texts.append(temp[5])\n",
        "        labels.append((float(temp[4])-range_min)/(range_max-range_min)) ## why ??  - normalize to range [0-1]\n",
        "        line=temp[5].strip()\n",
        "        sentences.append(indic_tokenize.trivial_tokenize(line))\n",
        "\n",
        "fp.close()\n"
      ],
      "metadata": {
        "id": "SbWfwj7Vjlzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer(num_words = MAX_NB_WORDS) #num_words=MAX_NB_WORDS) #limits vocabulory size\n",
        "tokenizer.fit_on_texts(texts) #encoding the text\n",
        "sequences=tokenizer.texts_to_sequences(texts) #returns list of sequences\n",
        "word_index=tokenizer.word_index #dictionary mapping, word and specific token for that word...\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) #padding to max_length\n",
        "\n",
        "\n",
        "print('Shape of data tensor:', data.shape)\n"
      ],
      "metadata": {
        "id": "X_6YdwwSjpJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"text labels appended %s\" %len(texts))\n",
        "\n",
        "labels=np.asarray(labels)\n",
        "print(labels)\n",
        "print(len(labels))"
      ],
      "metadata": {
        "id": "76FXWkyekUeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fast_emb = {}\n",
        "for i in sentences:\n",
        "  temp1 = np.zeros((1, EMBEDDING_DIM))\n",
        "  for w in i:\n",
        "    #print(w,np.asarray([float(j) for j in embedding_model[w]]))\n",
        "    fast_emb[w] = embedding_model[w]"
      ],
      "metadata": {
        "id": "sOtVE-hEkqx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(fast_emb)"
      ],
      "metadata": {
        "id": "yOL0WCxdkt2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices=np.arange(data.shape[0]) #with one argument, start=0, step =1\n",
        "print(data.shape)\n",
        "np.random.shuffle(indices)\n",
        "data=data[indices]\n",
        "print(data.shape)\n",
        "labels=labels[indices]\n",
        "# np.reshape(labels, ())\n",
        "print(labels.shape)\n",
        "validation_size=int(VALIDATION_SPLIT*data.shape[0])\n",
        "print(validation_size)"
      ],
      "metadata": {
        "id": "O7tzG9Vlj8e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=data[:-validation_size] #data-validation data\n",
        "print(x_train.shape)\n",
        "# print(x_train)\n",
        "# print(labels)\n",
        "y_train=labels[:-validation_size]\n",
        "# print(y_train.transpose)\n",
        "print(y_train.shape)\n",
        "# y_train = np.reshape(y_train, (1427, 1))\n",
        "# print(y_train_new)\n",
        "# print(y_train)\n",
        "x_val=data[-validation_size:]\n",
        "print(x_val.shape)\n",
        "y_val=labels[-validation_size:]"
      ],
      "metadata": {
        "id": "juUY7rH_lJjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((len(word_index), EMBEDDING_DIM))\n",
        "print(embedding_matrix.shape)"
      ],
      "metadata": {
        "id": "YS2mS4kblNRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word,i in word_index.items():\n",
        "\tif(i>=len(word_index)):\n",
        "\t\tcontinue\n",
        "\tif word in fast_emb:\n",
        "\t\t\tembedding_matrix[i]=fast_emb[word]\n",
        "vocab_size=len(word_index)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "12ctmL0MlQKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
        "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
        "\t\t\t\t\t\t\tmask_zero=True,\n",
        "\t\t\t\t\t\t\ttrainable=False)\n",
        "# print(embedding_layer.shape)\n",
        "side_embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
        "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
        "\t\t\t\t\t\t\tmask_zero=False,\n",
        "\t\t\t\t\t\t\ttrainable=False)"
      ],
      "metadata": {
        "id": "m64NUX_Ylg-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model = Sequential()\n",
        "forward_layer = LSTM(30, return_sequences=True)\n",
        "backward_layer = LSTM(30, activation='relu', return_sequences=True,\n",
        "                      go_backwards=True)\n",
        "model.add(Bidirectional(forward_layer, backward_layer=backward_layer,\n",
        "                        input_shape=inputshape)\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Bidirectional(LSTM(10)))\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(1, activation='leaky_relu'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
      ],
      "metadata": {
        "id": "RXoMBbIwlrCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss=\"mean_squared_error\",\n",
        "              metrics=['MSE'])"
      ],
      "metadata": {
        "id": "PaJrBSPcnk9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "JPETxHt8o5J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(sf_1)\n",
        "epochs = 100\n",
        "# epochs = 1000\n",
        "print(type(x_train))\n",
        "# y_train = np.asarray(y_train)\n",
        "print(type(y_train))\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=epochs, validation_data=([x_val], y_val))"
      ],
      "metadata": {
        "id": "jTPZNnofo80x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model.predict([x_val])\n",
        "y_pred"
      ],
      "metadata": {
        "id": "sW1_LG1sUUMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_fin = [int(round(a*(range_max-range_min)+range_min)) for a in y_val]\n",
        "print(y_val_fin)"
      ],
      "metadata": {
        "id": "vG17aK-VpWE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_fin =[int(round(a*(range_max-range_min)+range_min)) for a in y_pred.reshape(356).tolist()]\n",
        "print(y_pred_fin)"
      ],
      "metadata": {
        "id": "kPXaAxzrS00v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cohen_kappa_score(y_val_fin,y_pred_fin,weights=\"quadratic\"))"
      ],
      "metadata": {
        "id": "kA6-39yJTOpv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}