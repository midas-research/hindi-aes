{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEm7RfAlsW0c"
      },
      "outputs": [],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIgvqMSovn68"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yStp-YHhv4Bz"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpeElHqWwFSd"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRRXSXuwwIgo"
      },
      "outputs": [],
      "source": [
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES=r\"/content/indic_nlp_resources\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mdAtseqwQMC"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eskImZIwYoy"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/embedding-v2/indicnlp.ft.hi.300.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx221T8lwoa0"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "embedding_model = fasttext.load_model('/content/indicnlp.ft.hi.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxpbYgvbzhkz"
      },
      "outputs": [],
      "source": [
        "cd /content/calling-out-bluff/Model3(SkipFlow)/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iUjLjGawz-I"
      },
      "outputs": [],
      "source": [
        "import  keras.layers  as  klayers \n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, LSTM, Input, Embedding, GlobalAveragePooling1D, Concatenate, Activation, Lambda, BatchNormalization, Convolution1D, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import nltk\n",
        "from quadratic_weighted_kappa import QWK\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer, InputSpec\n",
        "from keras.optimizers import adam_v2\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import regularizers\n",
        "from keras import initializers\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "polheeBozLwd"
      },
      "outputs": [],
      "source": [
        "class Neural_Tensor_layer(Layer):\n",
        "\tdef __init__(self,output_dim,input_dim=None, **kwargs):\n",
        "\t\tself.output_dim=output_dim\n",
        "\t\tself.input_dim=input_dim\n",
        "\t\tif self.input_dim:\n",
        "\t\t\tkwargs['input_shape']=(self.input_dim,)\n",
        "# \t\tprint(\"YAYY\", input_dim, output_dim)\n",
        "\t\tsuper(Neural_Tensor_layer,self).__init__(**kwargs)\n",
        "\n",
        "\tdef call(self,inputs,mask=None):\n",
        "\t\te1=inputs[0]\n",
        "\t\te2=inputs[1]\n",
        "\t\tbatch_size=K.shape(e1)[0]\n",
        "\t\tk=self.output_dim\n",
        "\t\t\n",
        "\n",
        "\t\tfeed_forward=K.dot(K.concatenate([e1,e2]),self.V)\n",
        "\n",
        "\t\tbilinear_tensor_products = [ K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1) ]\n",
        "\n",
        "\t\tfor i in range(k)[1:]:\t\n",
        "\t\t\tbtp=K.sum((e2*K.dot(e1,self.W[i]))+self.b,axis=1)\n",
        "\t\t\tbilinear_tensor_products.append(btp)\n",
        "\n",
        "\t\tresult=K.tanh(K.reshape(K.concatenate(bilinear_tensor_products,axis=0),(batch_size,k))+feed_forward)\n",
        "\n",
        "\t\treturn result\n",
        "    \n",
        "\tdef build(self,input_shape):\n",
        "\t\tmean=0.0\n",
        "\t\tstd=1.0\n",
        "\t\tk=self.output_dim\n",
        "\t\td=self.input_dim\n",
        "\t\t##truncnorm generate continuous random numbers in given range\n",
        "\t\tW_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(k,d,d))\n",
        "\t\tV_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(2*d,k))\n",
        "\t\tself.W=K.variable(W_val)\n",
        "\t\tself.V=K.variable(V_val)\n",
        "\t\tself.b=K.zeros((self.input_dim,))\n",
        "\t\tself._trainable_weights=[self.W,self.V,self.b]    \n",
        "\n",
        "\tdef compute_output_shape(self, input_shape):\n",
        "\t\tbatch_size=input_shape[0][0]\n",
        "\t\treturn(batch_size,self.output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPF94FV-ztwN"
      },
      "outputs": [],
      "source": [
        "class Temporal_Mean_Pooling(Layer): # conversion from (samples,timesteps,features) to (samples,features)\n",
        "\tdef __init__(self, **kwargs):\n",
        "\t\tsuper(Temporal_Mean_Pooling,self).__init__(**kwargs)\n",
        "\t\t# masked values in x (number_of_samples,time)\n",
        "\t\tself.supports_masking=True\n",
        "\t\t# Specifies number of dimensions to each layer\n",
        "\t\tself.input_spec=InputSpec(ndim=3)\n",
        "        \n",
        "\tdef call(self,x,mask=None):\n",
        "\t\tif mask is None:\n",
        "\t\t\tmask=K.mean(K.ones_like(x),axis=-1)\n",
        "\n",
        "\t\tmask=K.cast(mask,K.floatx())\n",
        "\t\t\t\t#dimension size single vec/number of samples\n",
        "\t\treturn K.sum(x,axis=-2)/K.sum(mask,axis=-1,keepdims=True)        \n",
        "\n",
        "\tdef compute_mask(self,input,mask):\n",
        "\t\treturn None\n",
        "    \n",
        "    \n",
        "\tdef compute_output_shape(self,input_shape):\n",
        "\t\treturn (input_shape[0],input_shape[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NlZpBgizxlh"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM=300\n",
        "MAX_NB_WORDS=4000\n",
        "\n",
        "MAX_SEQUENCE_LENGTH=500\n",
        "VALIDATION_SPLIT=0.20\n",
        "DELTA=20\n",
        "\n",
        "texts=[]\n",
        "\n",
        "labels=[]\n",
        "\n",
        "\n",
        "originals = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxX59Xz9z7qZ"
      },
      "outputs": [],
      "source": [
        "essay_type = '2'\n",
        "\n",
        "fp=open(\"dataset.tsv\",'r', encoding=\"utf-8\", errors=\"ignore\")\n",
        "fp.readline()\n",
        "originals = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrm8gFyo0tqJ"
      },
      "outputs": [],
      "source": [
        "for line in fp:\n",
        "    temp=line.split(\"\\t\")\n",
        "    if(temp[3]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
        "        originals.append(float(temp[5]))\n",
        "# print(originals)\n",
        "fp.close()\n",
        "# print(originals)\n",
        "print(\"range min - \", min(originals) , \" ; range max - \", max(originals))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZqlnO9B3gJ1"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chTG8Sq41HFE"
      },
      "outputs": [],
      "source": [
        "from indicnlp.tokenize import indic_tokenize  \n",
        "\n",
        "range_min = min(originals)\n",
        "range_max = max(originals)\n",
        "\n",
        "fp=open(\"dataset.tsv\",'r', encoding=\"utf-8\", errors=\"ignore\")\n",
        "fp.readline()\n",
        "sentences=[]\n",
        "for line in fp:\n",
        "    temp=line.split(\"\\t\")\n",
        "    if(temp[3]==essay_type): ## why only 4 ?? - evals in prompt specific fashion\n",
        "        texts.append(temp[6])\n",
        "        labels.append((float(temp[5])-range_min)/(range_max-range_min)) ## why ??  - normalize to range [0-1]\n",
        "        line=temp[6].strip()\n",
        "        sentences.append(indic_tokenize.trivial_tokenize(line))\n",
        "\n",
        "fp.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIX-iO9R05uY"
      },
      "outputs": [],
      "source": [
        "#!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42OFLPl2yVSh"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
        "model = AutoModel.from_pretrained(\"ai4bharat/indic-bert\")\n",
        "input_ids = torch.tensor(tokenizer.encode(sen[:128])).unsqueeze(0)  # Batch size 1\n",
        "outputs = model(input_ids)\n",
        "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_zv_d-k1EtB"
      },
      "outputs": [],
      "source": [
        "last_hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkvf6Ytx3Zjo"
      },
      "outputs": [],
      "source": [
        "labels\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCgJbHxa3lVI"
      },
      "outputs": [],
      "source": [
        "texts[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2HO0Ab73nsF"
      },
      "outputs": [],
      "source": [
        "textso = texts.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPbt9lvT80N9"
      },
      "outputs": [],
      "source": [
        "print(\"text labels appended %s\" %len(texts))\n",
        "\n",
        "labels=np.asarray(labels)\n",
        "print(labels)\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6Sh8rFkeBIQ"
      },
      "outputs": [],
      "source": [
        "fast_emb = {}\n",
        "for i in sentences:\n",
        "  temp1 = np.zeros((1, EMBEDDING_DIM))\n",
        "  for w in i:\n",
        "    #print(w,np.asarray([float(j) for j in embedding_model[w]]))\n",
        "    fast_emb[w] = embedding_model[w]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEWIDwZ-_1ku"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OV9YSlbAH4A"
      },
      "outputs": [],
      "source": [
        "tokenized = tokenizer(texts)['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xewkK_JHBDA_"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for i in tokenized:\n",
        "  sequences.append(list(map(lambda k:[k], i)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0RgEH76BEd3"
      },
      "outputs": [],
      "source": [
        "word_index = tokenizer.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3c6-7GEBXTr"
      },
      "outputs": [],
      "source": [
        "print('Found %s unique tokens.' % len(word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yegUemfHC7m2"
      },
      "outputs": [],
      "source": [
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) #padding to max_length\n",
        "print('Shape of data tensor:', data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMYcT0suC_YT"
      },
      "outputs": [],
      "source": [
        "data = np.reshape(data,(1600,500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z44JZi0KDEO9"
      },
      "outputs": [],
      "source": [
        "indices=np.arange(data.shape[0]) #with one argument, start=0, step =1\n",
        "print(data.shape)\n",
        "np.random.shuffle(indices)\n",
        "data=data[indices]\n",
        "print(data.shape)\n",
        "labels=labels[indices]\n",
        "# np.reshape(labels, ())\n",
        "print(labels.shape)\n",
        "validation_size=int(VALIDATION_SPLIT*data.shape[0])\n",
        "print(validation_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLK53nqKDw1V"
      },
      "outputs": [],
      "source": [
        "x_train=data[:-validation_size] #data-validation data\n",
        "print(x_train.shape)\n",
        "# print(x_train)\n",
        "# print(labels)\n",
        "y_train=labels[:-validation_size]\n",
        "# print(y_train.transpose)\n",
        "print(y_train.shape)\n",
        "# y_train = np.reshape(y_train, (1427, 1))\n",
        "# print(y_train_new)\n",
        "# print(y_train)\n",
        "x_val=data[-validation_size:]\n",
        "print(x_val.shape)\n",
        "y_val=labels[-validation_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9dw1UNUD1it"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = np.zeros((len(word_index), EMBEDDING_DIM))\n",
        "print(embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBPcUipnD6Kh"
      },
      "outputs": [],
      "source": [
        "for word,i in word_index.items():\n",
        "\tif(i>=len(word_index)):\n",
        "\t\tcontinue\n",
        "\tif word in fast_emb:\n",
        "\t\t\tembedding_matrix[i]=fast_emb[word]\n",
        "vocab_size=len(word_index)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsVvO21ml-Dh"
      },
      "outputs": [],
      "source": [
        "for i in embedding_matrix:\n",
        "  if np.all(i != 0):\n",
        "    print(i)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2zUvTWUi3ge"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for word,i in word_index.items():\n",
        "  if(i>=len(word_index)):\n",
        "    continue\n",
        "  if word in fast_emb:\n",
        "    count = count + 1\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7978Y7ocED3z"
      },
      "outputs": [],
      "source": [
        "embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
        "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
        "\t\t\t\t\t\t\tmask_zero=True,\n",
        "\t\t\t\t\t\t\ttrainable=False)\n",
        "# print(embedding_layer.shape)\n",
        "side_embedding_layer=Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],\n",
        "\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
        "\t\t\t\t\t\t\tmask_zero=False,\n",
        "\t\t\t\t\t\t\ttrainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLlDilCgEK1g"
      },
      "outputs": [],
      "source": [
        "def SKIPFLOW(lstm_dim, lr, lr_decay, k, eta, delta, activation, maxlen=MAX_SEQUENCE_LENGTH, seed=None):\n",
        "    e = Input(name='essay',shape=(maxlen,))\n",
        "    print(\"e\", e)\n",
        "#     trad_feats=Input(shape=(7,))\n",
        "#     print(\"trad_feats\", trad_feats)\n",
        "    embed = embedding_layer(e)\n",
        "    print(embed.shape)\n",
        "    lstm_layer=LSTM(lstm_dim,return_sequences=True)\n",
        "    print(lstm_layer)\n",
        "    hidden_states=lstm_layer(embed)\n",
        "    htm=Temporal_Mean_Pooling()(hidden_states)    \n",
        "    side_embed = side_embedding_layer(e)\n",
        "    side_hidden_states=lstm_layer(side_embed)    \n",
        "    tensor_layer=Neural_Tensor_layer(output_dim=k,input_dim=lstm_dim)\n",
        "#     print(input_dim, output_dim)\n",
        "    pairs = [((eta + i * delta) % maxlen, (eta + i * delta + delta) % maxlen) for i in range(maxlen // delta)]\n",
        "    hidden_pairs = [ (Lambda(lambda t: t[:, p[0], :])(side_hidden_states), Lambda(lambda t: t[:, p[1], :])(side_hidden_states)) for p in pairs]\n",
        "    sigmoid = Dense(1, activation=\"sigmoid\", kernel_initializer=initializers.glorot_normal(seed=seed))\n",
        "    coherence = [sigmoid(tensor_layer([hp[0], hp[1]])) for hp in hidden_pairs]\n",
        "    co_tm=Concatenate()(coherence[:]+[htm])\n",
        "    dense = Dense(256, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(co_tm)\n",
        "    dense = Dense(128, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
        "    dense = Dense(64, activation=activation,kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
        "    out = Dense(1, activation=\"sigmoid\")(dense)\n",
        "    smodel = Model(inputs=[e], outputs=[out])\n",
        "    print(\"input\", [e])\n",
        "    print(\"outputs\", out)\n",
        "    adam = Adam(lr=lr, decay=lr_decay)\n",
        "    smodel.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=[\"MSE\"])\n",
        "    return smodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-lXyP3DEbxm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "earlystopping = EarlyStopping(monitor=\"val_MSE\", patience=5)\n",
        "sf_2 = SKIPFLOW(lstm_dim=500, lr=0.01, lr_decay=0, k=2, eta=13, delta=50, activation=\"relu\", seed=None)\n",
        "sf_2.summary()\n",
        "# plot_model(sf_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u2Pe6ZSTEnN8"
      },
      "outputs": [],
      "source": [
        "# print(sf_1)\n",
        "epochs = 8\n",
        "# epochs = 1000\n",
        "print(type(x_train))\n",
        "# y_train = np.asarray(y_train)\n",
        "print(type(y_train))\n",
        "\n",
        "sf_2.fit(x_train, y_train, batch_size=64, epochs=epochs, validation_data=([x_val], y_val), callbacks=[earlystopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VO2Ds0YGUDp"
      },
      "outputs": [],
      "source": [
        "y_pred=sf_2.predict([x_val])\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDjYl8CPHGPX"
      },
      "outputs": [],
      "source": [
        "y_val_fin = [int(round(a*(range_max-range_min)+range_min)) for a in y_val]\n",
        "print(y_val_fin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTnnx3hdHXm8"
      },
      "outputs": [],
      "source": [
        "y_pred_fin =[int(round(a*(range_max-range_min)+range_min)) for a in y_pred.reshape(320).tolist()]\n",
        "print(y_pred_fin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dco1sx6yHcjf"
      },
      "outputs": [],
      "source": [
        "print(cohen_kappa_score(y_val_fin,y_pred_fin,weights=\"quadratic\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxVs1gjGHi7V"
      },
      "outputs": [],
      "source": [
        "def Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Returns the confusion matrix between rater's ratings\n",
        "    \"\"\"\n",
        "    assert(len(rater_a) == len(rater_b))\n",
        "    if min_rating is None:\n",
        "        min_rating = min(rater_a + rater_b)\n",
        "    if max_rating is None:\n",
        "        max_rating = max(rater_a + rater_b)\n",
        "    num_ratings = int(max_rating - min_rating + 1)\n",
        "    conf_mat = [[0 for i in range(num_ratings)]\n",
        "                for j in range(num_ratings)]\n",
        "    for a, b in zip(rater_a, rater_b):\n",
        "        conf_mat[a - min_rating][b - min_rating] += 1\n",
        "    return conf_mat\n",
        "\n",
        "\n",
        "def histogram(ratings, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Returns the counts of each type of rating that a rater made\n",
        "    \"\"\"\n",
        "    if min_rating is None:\n",
        "        min_rating = min(ratings)\n",
        "    if max_rating is None:\n",
        "        max_rating = max(ratings)\n",
        "    num_ratings = int(max_rating - min_rating + 1)\n",
        "    hist_ratings = [0 for x in range(num_ratings)]\n",
        "    for r in ratings:\n",
        "        hist_ratings[r - min_rating] += 1\n",
        "    return hist_ratings\n",
        "\n",
        "\n",
        "def QWK_new(y, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the quadratic weighted kappa\n",
        "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
        "    value, which is a measure of inter-rater agreement between two raters\n",
        "    that provide discrete numeric ratings.  Potential values range from -1\n",
        "    (representing complete disagreement) to 1 (representing complete\n",
        "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
        "    chance.\n",
        "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
        "    each correspond to a list of integer ratings.  These lists must have the\n",
        "    same length.\n",
        "    The ratings should be integers, and it is assumed that they contain\n",
        "    the complete range of possible ratings.\n",
        "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
        "    is the minimum possible rating, and max_rating is the maximum possible\n",
        "    rating\n",
        "    \"\"\"\n",
        "    rater_a = y\n",
        "    rater_b = y_pred\n",
        "    min_rating=None\n",
        "    max_rating=None\n",
        "    rater_a = np.array(rater_a, dtype=int)\n",
        "    rater_b = np.array(rater_b, dtype=int)\n",
        "    assert(len(rater_a) == len(rater_b))\n",
        "    if min_rating is None:\n",
        "        min_rating = min(min(rater_a), min(rater_b))\n",
        "    if max_rating is None:\n",
        "        max_rating = max(max(rater_a), max(rater_b))\n",
        "    conf_mat = Cmatrix(rater_a, rater_b,\n",
        "                                min_rating, max_rating)\n",
        "    num_ratings = len(conf_mat)\n",
        "    num_scored_items = float(len(rater_a))\n",
        "\n",
        "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
        "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
        "\n",
        "    numerator = 0.0\n",
        "    denominator = 0.0\n",
        "\n",
        "    for i in range(num_ratings):\n",
        "        for j in range(num_ratings):\n",
        "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
        "                              / num_scored_items)\n",
        "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
        "            numerator += d * conf_mat[i][j] / num_scored_items\n",
        "            denominator += d * expected_count / num_scored_items\n",
        "\n",
        "    return (1.0 - numerator / denominator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVZPPjnJJrM5"
      },
      "outputs": [],
      "source": [
        "QWK_new(y_val_fin, y_pred_fin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_jLfJqVJt7d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "earlystopping = EarlyStopping(monitor=\"val_MSE\", patience=5)\n",
        "sf_2 = SKIPFLOW(lstm_dim=700, lr=0.01, lr_decay=0, k=2, eta=13, delta=50, activation=\"relu\", seed=None)\n",
        "sf_2.summary()\n",
        "# plot_model(sf_1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}